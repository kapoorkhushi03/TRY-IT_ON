{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kapoorkhushi03/TRY-IT_ON/blob/main/GeometricMatchingModule.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHj2ZEucOfe9",
        "outputId": "3cb8b510-2497-455c-9be9-806f455ecc6e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.10/dist-packages (0.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (24.1)\n",
            "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from tensorflow-addons) (2.13.3)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from opencv-python-headless) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "# Install TensorFlow\n",
        "!pip install tensorflow\n",
        "\n",
        "# Install TensorFlow Addons for additional functionalities (optional)\n",
        "!pip install tensorflow-addons\n",
        "\n",
        "# Install OpenCV for image processing\n",
        "!pip install opencv-python-headless\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# For loading datasets and creating data loaders\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.applications import VGG16  # or another pre-trained model\n",
        "from tensorflow.keras import layers, models"
      ],
      "metadata": {
        "id": "bifKwJgoO9Yq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load images\n",
        "img1_path = '/content/image_cloth.jpg'\n",
        "img2_path = '/content/image_mask2.jpg'\n",
        "\n",
        "img1 = cv2.imread(img1_path)\n",
        "img2 = cv2.imread(img2_path)\n",
        "\n",
        "# Convert images to grayscale\n",
        "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Resize images\n",
        "img1_resized = cv2.resize(img1_gray, (256, 256))\n",
        "img2_resized = cv2.resize(img2_gray, (256, 256))\n",
        "\n",
        "# Normalize images\n",
        "scaler = MinMaxScaler()\n",
        "img1_normalized = scaler.fit_transform(img1_resized)\n",
        "img2_normalized = scaler.transform(img2_resized)\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "img1_tensor = tf.convert_to_tensor(img1_normalized, dtype=tf.float32)\n",
        "img2_tensor = tf.convert_to_tensor(img2_normalized, dtype=tf.float32)\n",
        "\n",
        "# Add batch and channel dimensions\n",
        "img1_tensor = tf.expand_dims(img1_tensor, axis=0)  # Shape [1, 256, 256]\n",
        "img1_tensor = tf.expand_dims(img1_tensor, axis=-1)  # Shape [1, 256, 256, 1]\n",
        "\n",
        "img2_tensor = tf.expand_dims(img2_tensor, axis=0)  # Shape [1, 256, 256]\n",
        "img2_tensor = tf.expand_dims(img2_tensor, axis=-1)  # Shape [1, 256, 256, 1]\n",
        "\n",
        "# Print shapes to verify\n",
        "print(f'img1_tensor shape: {img1_tensor.shape}')\n",
        "print(f'img2_tensor shape: {img2_tensor.shape}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFUd9wanPHXf",
        "outputId": "27e3263b-efcd-403e-c3a6-7db12096bebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "img1_tensor shape: (1, 256, 256, 1)\n",
            "img2_tensor shape: (1, 256, 256, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Load and preprocess images\n",
        "img1_path = '/content/image_cloth.jpg'\n",
        "img2_path = '/content/image_mask2.jpg'\n",
        "\n",
        "img1 = cv2.imread(img1_path)\n",
        "img2 = cv2.imread(img2_path)\n",
        "\n",
        "# Convert images to grayscale\n",
        "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Resize images\n",
        "img1_resized = cv2.resize(img1_gray, (256, 256))\n",
        "img2_resized = cv2.resize(img2_gray, (256, 256))\n",
        "\n",
        "# Normalize images\n",
        "scaler = MinMaxScaler()\n",
        "img1_normalized = scaler.fit_transform(img1_resized)\n",
        "img2_normalized = scaler.transform(img2_resized)\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "img1_tensor = tf.convert_to_tensor(img1_normalized, dtype=tf.float32)\n",
        "img2_tensor = tf.convert_to_tensor(img2_normalized, dtype=tf.float32)\n",
        "\n",
        "# Add batch and channel dimensions\n",
        "img1_tensor = tf.expand_dims(img1_tensor, axis=0)  # Shape [1, 256, 256]\n",
        "img1_tensor = tf.expand_dims(img1_tensor, axis=-1)  # Shape [1, 256, 256, 1]\n",
        "\n",
        "img2_tensor = tf.expand_dims(img2_tensor, axis=0)  # Shape [1, 256, 256]\n",
        "img2_tensor = tf.expand_dims(img2_tensor, axis=-1)  # Shape [1, 256, 256, 1]\n",
        "\n",
        "# Create a dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((img1_tensor, img1_tensor)).batch(1)\n",
        "\n",
        "# Define the Autoencoder model\n",
        "class Autoencoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(16, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 16, 128, 128]\n",
        "            tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 32, 64, 64]\n",
        "            tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 64, 32, 32]\n",
        "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')  # [B, 128, 16, 16]\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 64, 32, 32]\n",
        "            tf.keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 32, 64, 64]\n",
        "            tf.keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 16, 128, 128]\n",
        "            tf.keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding='same', activation='sigmoid')  # [B, 1, 256, 256]\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Initialize model\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with the loss function and optimizer\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "# Train the autoencoder\n",
        "history = autoencoder.fit(\n",
        "    train_dataset,\n",
        "    epochs=10  # Set the number of epochs you want\n",
        ")\n",
        "\n",
        "# Print training history\n",
        "print(history.history)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4YzOcskPwxc",
        "outputId": "5e738ccd-cc87-4f70-e342-ec250c96a0f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 8s 8s/step - loss: 0.1762\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1759\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.1753\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 91ms/step - loss: 0.1744\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 100ms/step - loss: 0.1727\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 115ms/step - loss: 0.1699\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 99ms/step - loss: 0.1655\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 65ms/step - loss: 0.1586\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 78ms/step - loss: 0.1490\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.1372\n",
            "{'loss': [0.1762121021747589, 0.175878643989563, 0.1753210723400116, 0.17435383796691895, 0.17269492149353027, 0.16992491483688354, 0.16545391082763672, 0.15861624479293823, 0.14897778630256653, 0.1372307687997818]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define the Autoencoder model\n",
        "class Autoencoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        # Define the encoder part\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(16, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 16, 128, 128]\n",
        "            tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 32, 64, 64]\n",
        "            tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 64, 32, 32]\n",
        "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')  # [B, 128, 16, 16]\n",
        "        ])\n",
        "        # Define the decoder part\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 64, 32, 32]\n",
        "            tf.keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 32, 64, 64]\n",
        "            tf.keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 16, 128, 128]\n",
        "            tf.keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding='same', activation='sigmoid')  # [B, 1, 256, 256]\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Initialize the model\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Define the training loop\n",
        "num_epochs = 50\n",
        "for epoch in range(num_epochs):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass\n",
        "        outputs = autoencoder(img1_tensor, training=True)\n",
        "        # Compute loss\n",
        "        loss = loss_fn(img1_tensor, outputs)\n",
        "    # Compute gradients\n",
        "    gradients = tape.gradient(loss, autoencoder.trainable_variables)\n",
        "    # Update weights\n",
        "    optimizer.apply_gradients(zip(gradients, autoencoder.trainable_variables))\n",
        "\n",
        "    # Print the loss every 10 epochs\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJspWrCDQeiV",
        "outputId": "afc6a98b-6bad-444a-fcc3-89dfc453b015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/50], Loss: 0.1672\n",
            "Epoch [20/50], Loss: 0.1247\n",
            "Epoch [30/50], Loss: 0.0992\n",
            "Epoch [40/50], Loss: 0.0839\n",
            "Epoch [50/50], Loss: 0.0717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Define the Autoencoder model\n",
        "class Autoencoder(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2D(16, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 16, 128, 128]\n",
        "            tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 32, 64, 64]\n",
        "            tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 64, 32, 32]\n",
        "            tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding='same', activation='relu')  # [B, 128, 16, 16]\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Conv2DTranspose(64, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 64, 32, 32]\n",
        "            tf.keras.layers.Conv2DTranspose(32, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 32, 64, 64]\n",
        "            tf.keras.layers.Conv2DTranspose(16, kernel_size=3, strides=2, padding='same', activation='relu'),  # [B, 16, 128, 128]\n",
        "            tf.keras.layers.Conv2DTranspose(1, kernel_size=3, strides=2, padding='same', activation='sigmoid')  # [B, 1, 256, 256]\n",
        "        ])\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.encoder(x)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "    def encode(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "# Initialize the model\n",
        "autoencoder = Autoencoder()\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_fn = tf.keras.losses.MeanSquaredError()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model with the loss function and optimizer\n",
        "autoencoder.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "# Load and preprocess images\n",
        "img1_path = '/content/image_cloth.jpg'\n",
        "img2_path = '/content/image_mask2.jpg'\n",
        "\n",
        "img1 = cv2.imread(img1_path)\n",
        "img2 = cv2.imread(img2_path)\n",
        "\n",
        "# Convert images to grayscale\n",
        "img1_gray = cv2.cvtColor(img1, cv2.COLOR_BGR2GRAY)\n",
        "img2_gray = cv2.cvtColor(img2, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Resize images\n",
        "img1_resized = cv2.resize(img1_gray, (256, 256))\n",
        "img2_resized = cv2.resize(img2_gray, (256, 256))\n",
        "\n",
        "# Normalize images\n",
        "scaler = MinMaxScaler()\n",
        "img1_normalized = scaler.fit_transform(img1_resized)\n",
        "img2_normalized = scaler.transform(img2_resized)\n",
        "\n",
        "# Convert to TensorFlow tensors\n",
        "img1_tensor = tf.convert_to_tensor(img1_normalized, dtype=tf.float32)\n",
        "img2_tensor = tf.convert_to_tensor(img2_normalized, dtype=tf.float32)\n",
        "\n",
        "# Add batch and channel dimensions\n",
        "img1_tensor = tf.expand_dims(img1_tensor, axis=0)  # Shape [1, 256, 256]\n",
        "img1_tensor = tf.expand_dims(img1_tensor, axis=-1)  # Shape [1, 256, 256, 1]\n",
        "\n",
        "img2_tensor = tf.expand_dims(img2_tensor, axis=0)  # Shape [1, 256, 256]\n",
        "img2_tensor = tf.expand_dims(img2_tensor, axis=-1)  # Shape [1, 256, 256, 1]\n",
        "\n",
        "# Assuming you have trained the model before this step\n",
        "# Extract features from the images\n",
        "features_img1 = autoencoder.encode(img1_tensor).numpy()\n",
        "features_img2 = autoencoder.encode(img2_tensor).numpy()\n",
        "\n",
        "print(features_img1.shape)  # Should print something like [1, 16, 16, 128]\n",
        "print(features_img2.shape)  # Should print something like [1, 16, 16, 128]\n",
        "\n",
        "# If you need to remove the batch and channel dimensions\n",
        "features_img1 = np.squeeze(features_img1)  # [16, 16, 128]\n",
        "features_img2 = np.squeeze(features_img2)  # [16, 16, 128]\n",
        "\n",
        "print(features_img1.shape)  # Should print [16, 16, 128]\n",
        "print(features_img2.shape)  # Should print [16, 16, 128]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k4piF14XQ4CT",
        "outputId": "643ac66f-838c-4bd2-8d10-41635e1eb7f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16, 16, 128)\n",
            "(1, 16, 16, 128)\n",
            "(16, 16, 128)\n",
            "(16, 16, 128)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class FeatureCorrelation(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(FeatureCorrelation, self).__init__()\n",
        "\n",
        "    def call(self, feature_A, feature_B):\n",
        "        # Get the batch size, channels, height, and width\n",
        "        b, h, w, c = tf.shape(feature_A)[0], tf.shape(feature_A)[1], tf.shape(feature_A)[2], tf.shape(feature_A)[3]\n",
        "\n",
        "        # Reshape and transpose feature_A and feature_B\n",
        "        feature_A = tf.reshape(feature_A, [b, h*w, c])  # [B, H*W, C]\n",
        "        feature_B = tf.reshape(feature_B, [b, c, h*w])  # [B, C, H*W]\n",
        "        feature_A = tf.transpose(feature_A, [0, 2, 1])  # [B, C, H*W]\n",
        "\n",
        "        # Compute correlation\n",
        "        correlation = tf.matmul(feature_A, feature_B)  # [B, H*W, H*W]\n",
        "\n",
        "        # Reshape to [B, H, W, H, W]\n",
        "        correlation = tf.reshape(correlation, [b, h, w, h, w])\n",
        "\n",
        "        return correlation\n",
        "        print(\"Feature Correlation shape:\", correlation.shape)\n"
      ],
      "metadata": {
        "id": "GZPUXQIqRMz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class FeatureRegression(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "        self.conv3 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "        self.conv4 = tf.keras.layers.Conv2D(6, kernel_size=3, strides=1, padding='same')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "        print(\"Feature Regression output shape:\", output.shape)\n",
        "\n"
      ],
      "metadata": {
        "id": "BcKovkL2RzHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Define the FeatureCorrelation class\n",
        "class FeatureCorrelation(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(FeatureCorrelation, self).__init__()\n",
        "\n",
        "    def call(self, feature_A, feature_B):\n",
        "        # No need to squeeze here as the dimensions are already correct\n",
        "        b, h, w, c = tf.shape(feature_A)[0], tf.shape(feature_A)[1], tf.shape(feature_A)[2], tf.shape(feature_A)[3]\n",
        "        feature_A = tf.reshape(feature_A, [b, h*w, c])  # [B, H*W, C]\n",
        "        feature_B = tf.reshape(feature_B, [b, c, h*w])  # [B, C, H*W]\n",
        "        # feature_A = tf.transpose(feature_A, [0, 2, 1])  # [B, C, H*W]\n",
        "\n",
        "        correlation = tf.matmul(feature_A, feature_B)  # [B, H*W, H*W]\n",
        "        correlation = tf.reshape(correlation, [b, h, w, h, w])  # [B, H, W, H, W]\n",
        "\n",
        "        return correlation\n",
        "\n",
        "# ... (rest of the code remains unchanged)\n",
        "\n",
        "# Define the FeatureRegression class\n",
        "class FeatureRegression(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super(FeatureRegression, self).__init__()\n",
        "        self.conv1 = tf.keras.layers.Conv2D(128, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "        self.conv2 = tf.keras.layers.Conv2D(64, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "        self.conv3 = tf.keras.layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu')\n",
        "        self.conv4 = tf.keras.layers.Conv2D(6, kernel_size=3, strides=1, padding='same')\n",
        "\n",
        "    def call(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        return x\n",
        "\n",
        "# Assuming features_img1 and features_img2 are numpy arrays of shape [128, 16, 16]\n",
        "# Convert features to tensors\n",
        "feature_A = tf.convert_to_tensor(features_img1, dtype=tf.float32)\n",
        "feature_B = tf.convert_to_tensor(features_img2, dtype=tf.float32)\n",
        "\n",
        "# Add batch dimension\n",
        "feature_A = tf.expand_dims(feature_A, axis=0)  # [1, 16, 16, 128]\n",
        "feature_B = tf.expand_dims(feature_B, axis=0)  # [1, 16, 16, 128]\n",
        "\n",
        "# Compute correlation map\n",
        "correlation_layer = FeatureCorrelation()\n",
        "correlation_map = correlation_layer(feature_A, feature_B)\n",
        "\n",
        "print(correlation_map.shape)\n",
        "\n",
        "# Initialize regression layer\n",
        "regression_layer = FeatureRegression()\n",
        "\n",
        "# Flatten and reshape correlation map\n",
        "correlation_map_flattened = tf.reshape(correlation_map, [1, 256, 16, 16])  # [1, 256, 16, 16]\n",
        "\n",
        "# Compute theta from correlation map\n",
        "theta = regression_layer(correlation_map_flattened)\n",
        "\n",
        "print(theta.shape)  # Should print (1, 6, 16, 16)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_x8AS1m6R9-u",
        "outputId": "5c0cf4c6-87b3-4db8-f31f-71c0dff9b6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 16, 16, 16, 16)\n",
            "(1, 256, 16, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assume `img1_tensor` (PyTorch tensor) and `theta` (transformation parameters) are already defined\n",
        "\n",
        "# Convert img1_tensor (PyTorch tensor) to NumPy array\n",
        "img1_tensor_np = img1_tensor.numpy()  # Convert PyTorch tensor to NumPy array\n",
        "\n",
        "# Convert NumPy array to TensorFlow tensor\n",
        "img1_tensor_tf = tf.convert_to_tensor(img1_tensor_np, dtype=tf.float32)\n",
        "\n",
        "# Add batch and channel dimensions to the TensorFlow tensor if not present\n",
        "img1_tensor_tf = tf.expand_dims(img1_tensor_tf, axis=0)  # Add batch dimension\n",
        "img1_tensor_tf = tf.expand_dims(img1_tensor_tf, axis=-1)  # Add channel dimension\n",
        "\n",
        "# Define the transformation function\n",
        "def apply_transform(image, theta):\n",
        "    # Ensure theta is a 1D tensor with 6 elements\n",
        "    if theta.shape != (6,):\n",
        "        raise ValueError(f'Expected theta to be of shape (6,), but got {theta.shape}')\n",
        "\n",
        "    # Define the affine transformation matrix for TensorFlow\n",
        "    theta_tf = tf.reshape(theta, [6])  # Flatten theta to [6]\n",
        "\n",
        "    # Apply the affine transformation\n",
        "    transformed_image = tf.image.transform(\n",
        "        image,\n",
        "        theta_tf,\n",
        "        interpolation='BILINEAR',\n",
        "        fill_mode='CONSTANT',\n",
        "        fill_value=0.0,\n",
        "        output_shape=[256, 256]  # Change this as needed for your use case\n",
        "    )\n",
        "\n",
        "    return transformed_image\n",
        "\n",
        "# Sample theta for an identity transformation (modify as needed)\n",
        "theta = tf.constant([1, 0, 0, 0, 1, 0], dtype=tf.float32)  # Identity transformation\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_img1 = apply_transform(img1_tensor_tf, theta)\n",
        "\n",
        "# Convert tensor to NumPy array for visualization\n",
        "transformed_img1_np = tf.squeeze(transformed_img1).numpy()\n",
        "\n",
        "# Print tensor shapes for debugging\n",
        "print(f'img1_tensor_tf shape: {img1_tensor_tf.shape}')\n",
        "print(f'theta shape: {theta.shape}')\n",
        "print(f'transformed_img1 shape: {transformed_img1.shape}')\n",
        "\n",
        "# Visualize the transformed image\n",
        "plt.imshow(transformed_img1_np, cmap='gray')\n",
        "plt.title('Transformed Image')\n",
        "plt.axis('off')  # Optional: Hide axis for better visualization\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "MkuLgzPAhiX3",
        "outputId": "5bbf857b-5342-4360-d2bd-547aba6bf1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow._api.v2.image' has no attribute 'transform'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-d4873ce612b8>\u001b[0m in \u001b[0;36m<cell line: 42>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Apply the transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mtransformed_img1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg1_tensor_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# Convert tensor to NumPy array for visualization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-d4873ce612b8>\u001b[0m in \u001b[0;36mapply_transform\u001b[0;34m(image, theta)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Apply the affine transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     transformed_image = tf.image.transform(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtheta_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.image' has no attribute 'transform'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Assume `img1_tensor` (PyTorch tensor) and `theta` (transformation parameters) are already defined\n",
        "\n",
        "# Convert img1_tensor (PyTorch tensor) to NumPy array\n",
        "img1_tensor_np = tf.squeeze(img1_tensor).numpy()\n",
        "# Convert NumPy array to TensorFlow tensor\n",
        "img1_tensor_tf = tf.convert_to_tensor(img1_tensor_np, dtype=tf.float32)\n",
        "\n",
        "# Add batch and channel dimensions to the TensorFlow tensor if not present\n",
        "# Add batch and channel dimensions to the TensorFlow tensor if not present\n",
        "img1_tensor_tf = tf.expand_dims(img1_tensor_tf, axis=0)  # Add batch dimension\n",
        "img1_tensor_tf = tf.expand_dims(img1_tensor_tf, axis=-1)  # Add channel dimension\n",
        "\n",
        "\n",
        "# Define the transformation function\n",
        "def apply_transform(image, theta):\n",
        "    # Convert theta to 2D affine matrices for transformations\n",
        "    theta = tf.reshape(theta, [-1, 2, 3])  # [1, 2, 3]\n",
        "\n",
        "    # Create a grid for affine transformation\n",
        "    grid = tf.image.affine_transform(\n",
        "        image,\n",
        "        theta[0, :, :],  # Use the first (and only) batch element's theta\n",
        "        interpolation='BILINEAR',\n",
        "        fill_mode='CONSTANT',\n",
        "        fill_value=0.0,\n",
        "        output_shape=[256, 256]\n",
        "    )\n",
        "\n",
        "    return grid\n",
        "\n",
        "# Apply the transformation\n",
        "transformed_img1 = apply_transform(img1_tensor_tf, theta)\n",
        "\n",
        "# Convert tensor to NumPy array for visualization\n",
        "transformed_img1_np = tf.squeeze(transformed_img1).numpy()\n",
        "\n",
        "# Print tensor shapes for debugging\n",
        "print(f'img1_tensor_tf shape: {img1_tensor_tf.shape}')\n",
        "print(f'theta shape: {theta.shape}')\n",
        "print(f'transformed_img1 shape: {transformed_img1.shape}')\n",
        "\n",
        "# Visualize the transformed image\n",
        "plt.imshow(transformed_img1_np, cmap='gray')\n",
        "plt.title('Transformed Image')\n",
        "plt.axis('off')  # Optional: Hide axis for better visualization\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "HZx3ELHTb0hU",
        "outputId": "6ad3d4c5-073f-40ba-80ef-7343b5643c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'squeeze'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-8c93720fdaa0>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Convert img1_tensor (PyTorch tensor) to NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimg1_tensor_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg1_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Convert NumPy array to TensorFlow tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_enable_numpy_behavior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m       \"\"\")\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object has no attribute 'squeeze'"
          ]
        }
      ]
    }
  ]
}